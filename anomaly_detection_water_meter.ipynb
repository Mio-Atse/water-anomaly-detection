{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMVO8evK-hHJ"
      },
      "source": [
        "# Anomaly Detection with using PYOD and ADTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv2hH3I6-fHY"
      },
      "outputs": [],
      "source": [
        "!pip install pyod adtk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeIo5gXI-8_-"
      },
      "source": [
        "Connect your Google Drive account and transfer the dataset wherever you want. Make sure this folder is not used.\n",
        "After the connection, unzip the zip file and make sure that the dataset is unzipped as desired. The final file format should be as follows:\n",
        "\n",
        "> your_path/dataset/\n",
        "\n",
        "\n",
        "          ->../helios\n",
        "            ->../user_helios_sorted\n",
        "          ->../queensland\n",
        "            ->../user_sorted_pulse\n",
        "            ->../user_sorted_pulsetot\n",
        "          ->../datamill\n",
        "              ->../user_datamill_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2qJLCXD-vel",
        "outputId": "5e5793b2-dd86-421e-f2e2-f5a8215033c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmuYlsDIEJGL"
      },
      "outputs": [],
      "source": [
        "## go to your path\n",
        "%cd /content/drive/MyDrive/kentkart/water_meter_dataset/ ##change this part with your dataset location\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bXCoh1JwEeCO"
      },
      "outputs": [],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwAMKy6aAINN"
      },
      "source": [
        "##Anomaly Detection with ADTK without Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxg9h20gANI0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from adtk.data import validate_series\n",
        "from adtk.detector import ThresholdAD, InterQuartileRangeAD, PersistAD, LevelShiftAD, VolatilityShiftAD\n",
        "\n",
        "def process_datamill(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['READING_START_DATE'] = pd.to_datetime(df['READING_START_DATE'], format='%d/%m/%Y %H:%M')\n",
        "    df = df.set_index('READING_START_DATE')\n",
        "    return df['GROSS_CONSUMPTION']\n",
        "\n",
        "def process_helios(file_path, option):\n",
        "    df = pd.read_csv(file_path, sep=';')\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "    df = df.set_index('datetime')\n",
        "    if option == 'daily':\n",
        "        return df['diff']\n",
        "    else:  # total\n",
        "        return df['meter reading']\n",
        "\n",
        "def process_queensland(file_path, option):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "    df = df.set_index('datetime')\n",
        "    if option == 'pulse1':\n",
        "        return df['Pulse1']\n",
        "    else:  # pulse1_total\n",
        "        return df['Pulse1_Total']\n",
        "\n",
        "def detect_anomalies(s, contamination, z_score_threshold):\n",
        "    s = validate_series(s)\n",
        "\n",
        "    threshold_ad = ThresholdAD(high=s.mean() + z_score_threshold * s.std(),\n",
        "                               low=s.mean() - z_score_threshold * s.std())\n",
        "    iqr_ad = InterQuartileRangeAD(c=1.5)\n",
        "    persist_ad = PersistAD(c=3.0, side='positive')\n",
        "    level_shift_ad = LevelShiftAD(c=2.0, side='both', window=5)\n",
        "    volatility_shift_ad = VolatilityShiftAD(c=1.5, side='positive', window=30)\n",
        "\n",
        "    anomalies = {}\n",
        "    anomalies['Threshold'] = threshold_ad.detect(s).fillna(False)\n",
        "    anomalies['IQR'] = iqr_ad.fit_detect(s).fillna(False)\n",
        "    anomalies['Persist'] = persist_ad.fit_detect(s).fillna(False)\n",
        "    anomalies['LevelShift'] = level_shift_ad.fit_detect(s).fillna(False)\n",
        "    try:\n",
        "        anomalies['VolatilityShift'] = volatility_shift_ad.fit_detect(s).fillna(False)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"VolatilityShiftAD could not be applied: {e}\")\n",
        "        anomalies['VolatilityShift'] = pd.Series(False, index=s.index)\n",
        "\n",
        "    return anomalies\n",
        "\n",
        "\n",
        "def plot_consensus_anomalies(s, anomalies, file_path):\n",
        "    # Calculate the consensus anomalies where all models agree\n",
        "    consensus_anomalies = pd.DataFrame(anomalies).all(axis=1)\n",
        "\n",
        "    # Drop any missing values to avoid index alignment issues\n",
        "    consensus_anomalies = consensus_anomalies.dropna()\n",
        "    s = s.dropna()\n",
        "\n",
        "    # Filter the consensus anomalies\n",
        "    consensus_indices = consensus_anomalies[consensus_anomalies].index\n",
        "\n",
        "    # Ensure indices in s align with the consensus_indices\n",
        "    consensus_values = s.loc[consensus_indices]\n",
        "\n",
        "    # Check if both consensus_indices and consensus_values have the same length\n",
        "    if len(consensus_indices) != len(consensus_values):\n",
        "        print(f\"Index mismatch detected. Length of indices: {len(consensus_indices)}, Length of values: {len(consensus_values)}\")\n",
        "        return\n",
        "\n",
        "    # Plot the data and anomalies\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(s, label='Data', color='blue')\n",
        "    plt.scatter(consensus_indices,\n",
        "                consensus_values,\n",
        "                label='Consensus Anomaly',\n",
        "                marker='o',\n",
        "                color='red',\n",
        "                s=20)\n",
        "    plt.title(f\"Consensus Anomalies detected in {os.path.basename(file_path)}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main(folder_path, dataset_type, option, contamination, z_score_threshold):\n",
        "    file_list = glob.glob(os.path.join(folder_path, '*.csv'))\n",
        "\n",
        "    for file_path in file_list:\n",
        "        print(f\"Processing file: {file_path}\")\n",
        "\n",
        "        if dataset_type == 'datamill':\n",
        "            s = process_datamill(file_path)\n",
        "        elif dataset_type == 'helios':\n",
        "            s = process_helios(file_path, option)\n",
        "        elif dataset_type == 'queensland':\n",
        "            s = process_queensland(file_path, option)\n",
        "\n",
        "        anomalies = detect_anomalies(s, contamination, z_score_threshold)\n",
        "\n",
        "        total_consensus_anomalies = sum(pd.DataFrame(anomalies).all(axis=1))\n",
        "        print(f\"Total consensus anomalies across all models: {total_consensus_anomalies}\")\n",
        "\n",
        "        plot_consensus_anomalies(s, anomalies, file_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_type = input(\"Enter dataset type (helios/queensland/datamill): \").lower()\n",
        "    while dataset_type not in ['helios', 'queensland', 'datamill']:\n",
        "        dataset_type = input(\"Invalid input. Please enter 'helios', 'queensland', or 'datamill': \").lower()\n",
        "\n",
        "    if dataset_type == 'helios':\n",
        "        option = input(\"Enter option (daily/total): \").lower()\n",
        "        while option not in ['daily', 'total']:\n",
        "            option = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "    elif dataset_type == 'queensland':\n",
        "        option = input(\"Enter option (pulse1/pulse1_total): \").lower()\n",
        "        while option not in ['pulse1', 'pulse1_total']:\n",
        "            option = input(\"Invalid input. Please enter 'pulse1' or 'pulse1_total': \").lower()\n",
        "    else:\n",
        "        option = 'default'\n",
        "\n",
        "    try:\n",
        "        z_score_threshold = float(input(\"Enter Z-score threshold (default 1 for datamill and helios, 3 for queensland): \"))\n",
        "    except ValueError:\n",
        "        print(\"Invalid Z-score threshold. Using default value of 3.\")\n",
        "        z_score_threshold = 3\n",
        "\n",
        "    try:\n",
        "        contamination = float(input(\"Enter contamination factor (default 0.01): \"))\n",
        "    except ValueError:\n",
        "        print(\"Invalid contamination factor. Using default value of 0.01.\")\n",
        "        contamination = 0.01\n",
        "\n",
        "    if dataset_type == 'helios':\n",
        "\n",
        "        folder_path = './dataset/helios/user_helios_sorted/'\n",
        "    elif dataset_type == 'queensland':\n",
        "        if option == 'pulse1':\n",
        "\n",
        "            folder_path = './dataset/queensland/user_sorted_pulse/'\n",
        "        else:\n",
        "\n",
        "            folder_path = './dataset/queensland/user_sorted_pulsetot/'\n",
        "    else:\n",
        "\n",
        "        folder_path = './dataset/datamill/user_datamill_sorted/'\n",
        "\n",
        "    main(folder_path, dataset_type, option, contamination, z_score_threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRFSnzGBw5s"
      },
      "source": [
        "##Anomaly Detection with PYOD with single file training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAGfI9O-B4ub"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def process_file(file_path, dataset_type, option, contamination=0.01, models=None, z_score_threshold=3):\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path, sep=';') if 'helios' in dataset_type else pd.read_csv(file_path)\n",
        "\n",
        "    # Convert datetime to pandas datetime\n",
        "    if dataset_type == 'helios':\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "        if option == 'daily':\n",
        "            df['diff'] = df['diff']\n",
        "        elif option == 'total':\n",
        "            df['diff'] = df['meter reading'].diff()\n",
        "\n",
        "    elif dataset_type == 'queensland':\n",
        "        try:\n",
        "            df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "            if option == 'pulse1':\n",
        "                df['diff'] = df['Pulse1'].diff()\n",
        "            elif option == 'pulse1_total':\n",
        "                df['diff'] = df['Pulse1_Total'].diff()\n",
        "        except ValueError as e:\n",
        "            print(f\"Error converting datetime: {e}\")\n",
        "            print(f\"First few datetime values: {df['datetime'].head()}\")\n",
        "            raise\n",
        "\n",
        "    elif dataset_type == 'datamill':\n",
        "        df['datetime'] = pd.to_datetime(df['READING_START_DATE'], format='%d/%m/%Y %H:%M')\n",
        "        df['diff'] = df['GROSS_CONSUMPTION'].diff()\n",
        "\n",
        "    # Sort by datetime\n",
        "    df = df.sort_values('datetime')\n",
        "\n",
        "    # Extract hour and day of week\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "\n",
        "    # Calculate rolling statistics\n",
        "    window_size = 24 if dataset_type == 'helios' else 7\n",
        "    df['rolling_mean'] = df['diff'].rolling(window=window_size).mean()\n",
        "    df['rolling_std'] = df['diff'].rolling(window=window_size).std()\n",
        "\n",
        "    # Calculate Z-scores\n",
        "    df['z_score'] = (df['diff'] - df['rolling_mean']) / df['rolling_std']\n",
        "\n",
        "    # Prepare features for anomaly detection\n",
        "    features = ['diff', 'hour', 'day_of_week', 'rolling_mean', 'rolling_std']\n",
        "\n",
        "    # Handle NaN values\n",
        "    df[features] = df[features].fillna(df[features].mean())\n",
        "\n",
        "    X = df[features].values\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Initialize results dictionary\n",
        "    results = {}\n",
        "\n",
        "    # Anomaly Detection Models\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_scaled)\n",
        "        outlier_scores = model.decision_function(X_scaled)\n",
        "        df[f'{model_name}_anomaly_score'] = outlier_scores\n",
        "        df[f'{model_name}_is_anomaly'] = model.predict(X_scaled)\n",
        "        df[f'{model_name}_is_validated_anomaly'] = (df[f'{model_name}_is_anomaly'] == 1) & (abs(df['z_score']) > z_score_threshold)\n",
        "        results[model_name] = df[f'{model_name}_is_validated_anomaly'].sum()\n",
        "\n",
        "    return df, results\n",
        "\n",
        "def plot_results(df, user_key, dataset_type, results):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = df['datetime']\n",
        "    y = df['diff']\n",
        "    title = f'Validated Anomalies for {user_key} ({dataset_type})'\n",
        "\n",
        "    plt.plot(x, y, label='Consumption', alpha=0.5)\n",
        "\n",
        "    for model_name in results:\n",
        "        validated_anomalies = df[df[f'{model_name}_is_validated_anomaly']]\n",
        "        plt.scatter(validated_anomalies[x.name], validated_anomalies[y.name], label=f'{model_name} Validated Anomalies')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('DateTime')\n",
        "    plt.ylabel('Consumption Difference')\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main(folder_path, dataset_type, option, contamination, z_score_threshold):\n",
        "    # Define models\n",
        "    models = {\n",
        "        'IForest': IForest(contamination=contamination, random_state=42),\n",
        "        'KNN': KNN(contamination=contamination, n_neighbors=5),\n",
        "        'LOF': LOF(contamination=contamination),\n",
        "        'AutoEncoder': AutoEncoder(contamination=contamination, epoch_num=10)\n",
        "    }\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.csv'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            df, results = process_file(file_path, dataset_type, option, contamination, models, z_score_threshold)\n",
        "\n",
        "            user_key = filename\n",
        "\n",
        "            print(f\"Processing data for file: {filename}\")\n",
        "            print(f\"Total data points: {len(df)}\")\n",
        "            for model_name, count in results.items():\n",
        "                print(f\"{model_name} validated anomalies: {count}\")\n",
        "\n",
        "            # Plot the results\n",
        "            plot_results(df, user_key, dataset_type, results)\n",
        "\n",
        "# Get user input for dataset type\n",
        "dataset_type = input(\"Enter dataset type (helios/queensland/datamill): \").lower()\n",
        "while dataset_type not in ['helios', 'queensland', 'datamill']:\n",
        "    dataset_type = input(\"Invalid input. Please enter 'helios', 'queensland', or 'datamill': \").lower()\n",
        "\n",
        "# Get user input for options\n",
        "if dataset_type == 'helios':\n",
        "    option = input(\"Enter option (daily/total): \").lower()\n",
        "    while option not in ['daily', 'total']:\n",
        "        option = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "elif dataset_type == 'queensland':\n",
        "    option = input(\"Enter option (pulse1/pulse1_total): \").lower()\n",
        "    while option not in ['pulse1', 'pulse1_total']:\n",
        "        option = input(\"Invalid input. Please enter 'pulse1' or 'pulse1_total': \").lower()\n",
        "else:\n",
        "    option = 'default'\n",
        "\n",
        "# Get user input for Z-score and contamination\n",
        "try:\n",
        "    z_score_threshold = float(input(\"Enter Z-score threshold (default 1 for datamill and helios 3 for queensland): \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid Z-score threshold. Using default value of 3.\")\n",
        "    z_score_threshold = 3\n",
        "\n",
        "try:\n",
        "    contamination = float(input(\"Enter contamination factor (default 0.01): \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid contamination factor. Using default value of 0.01.\")\n",
        "    contamination = 0.01\n",
        "\n",
        "# Set the folder path based on the dataset type\n",
        "if dataset_type == 'helios':\n",
        "    #change this to your path,\n",
        "        #For example:\n",
        "        #folder path = /content/drive/MyDrive/your_path/dataset/helios/user_helios_sorted/'\n",
        "        folder_path = './dataset/helios/user_helios_sorted/'\n",
        "elif dataset_type == 'queensland':\n",
        "    if option == 'pulse1':\n",
        "\n",
        "            folder_path = './dataset/queensland/user_sorted_pulse/'\n",
        "    else:\n",
        "\n",
        "            folder_path = './dataset/queensland/user_sorted_pulsetot/'\n",
        "else:\n",
        "\n",
        "    folder_path = './dataset/datamill/user_datamill_sorted/'\n",
        "\n",
        "main(folder_path, dataset_type, option, contamination, z_score_threshold)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7N66eoDoYK"
      },
      "source": [
        "##Training Whole Dataset for Anomaly Detection\n",
        "\n",
        "The LOF model for the Helios dataset may take a long time or may run out of RAM. You can cancel long running models with the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gExECUEDD5hJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "def process_file(file_path, value_column, contamination=0.01):\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        if value_column == 'GROSS_CONSUMPTION' or value_column =='DAILY_AVERAGE_CONSUMPTION':\n",
        "            df['datetime'] = pd.to_datetime(df['READING_START_DATE'], format='%d/%m/%Y %H:%M')\n",
        "        else:\n",
        "\n",
        "            df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "        df = df.sort_values('datetime')\n",
        "\n",
        "        print(f\"DataFrame shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns}\")\n",
        "\n",
        "        # Extract hour and day of week\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "\n",
        "        # Calculate rolling statistics\n",
        "        window_size = 7\n",
        "        df['diff'] = df[value_column].diff()\n",
        "        df['rolling_mean'] = df['diff'].rolling(window=window_size).mean()\n",
        "        df['rolling_std'] = df['diff'].rolling(window=window_size).std()\n",
        "\n",
        "        # Calculate Z-scores\n",
        "        df['z_score'] = (df['diff'] - df['rolling_mean']) / df['rolling_std']\n",
        "\n",
        "        # Prepare features for anomaly detection\n",
        "        features = ['diff', 'day_of_week', 'rolling_mean', 'rolling_std']\n",
        "\n",
        "        # Handle NaN values\n",
        "        df[features] = df[features].fillna(df[features].mean())\n",
        "\n",
        "        # Check for remaining NaN values\n",
        "        if df[features].isnull().values.any():\n",
        "            print(\"NaN values found after filling with mean:\")\n",
        "            print(df[features].isnull().sum())\n",
        "            df[features] = df[features].fillna(0)  # Fallback to filling NaNs with 0 if any are left\n",
        "\n",
        "        X = df[features].values\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def train_and_save_models(folder_path, value_column, model_save_path, contamination=0.01):\n",
        "    print(f\"Starting training with folder_path: {folder_path}\")\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'IForest': IForest(contamination=contamination, random_state=42),\n",
        "        'KNN': KNN(contamination=contamination, n_neighbors=5),\n",
        "        'LOF': LOF(contamination=contamination),\n",
        "        'AutoEncoder': AutoEncoder(contamination=contamination)\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        file_list = os.listdir(folder_path)\n",
        "        print(f\"Files in directory: {file_list}\")\n",
        "\n",
        "        X_combined = []\n",
        "\n",
        "        for filename in file_list:\n",
        "            if filename.endswith('.csv'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "                X_scaled = process_file(file_path, value_column, contamination)\n",
        "                if X_scaled is not None:\n",
        "                    X_combined.append(X_scaled)\n",
        "\n",
        "        if X_combined:\n",
        "            X_combined = np.vstack(X_combined)\n",
        "            print(f\"Combined feature matrix shape: {X_combined.shape}\")\n",
        "\n",
        "            # Ensure the model save path exists\n",
        "            os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "            for model_name, model in models.items():\n",
        "                model.fit(X_combined)\n",
        "                model_filename = os.path.join(model_save_path, f'{model_name}_model.pkl')\n",
        "                joblib.dump(model, model_filename)\n",
        "                print(f\"Saved {model_name} model to {model_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in training and saving models: {str(e)}\")\n",
        "\n",
        "# Set the dataset type and value column based on the user's choice\n",
        "dataset_type = input(\"Enter dataset type (helios/queensland/datamill): \").lower()\n",
        "while dataset_type not in ['helios', 'queensland', 'datamill']:\n",
        "    dataset_type = input(\"Invalid input. Please enter 'helios', 'queensland', or 'datamill': \").lower()\n",
        "\n",
        "if dataset_type == 'helios':\n",
        "\n",
        "    folder_path = './dataset/helios/user_helios_sorted'\n",
        "\n",
        "    value_type = input(\"Enter data type (daily/total): \").lower()\n",
        "\n",
        "    while value_type not in ['daily', 'total']:\n",
        "        value_type = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "\n",
        "    if value_type == 'daily':\n",
        "        model_save_path = './models/helios/daily'\n",
        "        value_column = 'diff'\n",
        "    else:\n",
        "        model_save_path = './models/helios/total'\n",
        "        value_column = 'meter reading'\n",
        "\n",
        "elif dataset_type == 'queensland':\n",
        "\n",
        "    value_type = input(\"Enter data type (daily/total): \").lower()\n",
        "\n",
        "    while value_type not in ['daily', 'total']:\n",
        "        value_type = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "\n",
        "    if value_type == 'daily':\n",
        "        folder_path = './dataset/queensland/user_sorted_pulse'\n",
        "        model_save_path = './models/queensland/daily'\n",
        "        value_column = 'Pulse1'\n",
        "    else:\n",
        "        folder_path = './dataset/queensland/user_sorted_pulsetot'\n",
        "        model_save_path = './models/queensland/total'\n",
        "        value_column = 'Pulse1_Total'\n",
        "\n",
        "else:\n",
        "    folder_path = './dataset/datamill/user_datamill_sorted'  # Replace with the actual path\n",
        "    value_type = input(\"Enter data type (daily/total): \").lower()\n",
        "    while value_type not in ['daily', 'total']:\n",
        "        value_type = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "\n",
        "    if value_type == 'daily':\n",
        "        model_save_path = './models/datamill/daily/'\n",
        "        value_column = 'DAILY_AVERAGE_CONSUMPTION'\n",
        "    else:\n",
        "        model_save_path = './models/datamill/total/'\n",
        "        value_column = 'GROSS_CONSUMPTION'\n",
        "\n",
        "print(f\"Selected dataset type: {dataset_type}\")\n",
        "print(f\"Selected pulse type: {value_column}\")\n",
        "print(f\"Folder path: {folder_path}\")\n",
        "print(f\"Model save path: {model_save_path}\")\n",
        "\n",
        "try:\n",
        "    train_and_save_models(folder_path, value_column, model_save_path, contamination=0.01)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction with Trained Models"
      ],
      "metadata": {
        "id": "zMiJL6ZVoCdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyod.models.iforest import IForest\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.lof import LOF\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "def load_models(models_path):\n",
        "    models = {}\n",
        "    for model_name in ['IForest_model', 'KNN_model', 'LOF_model', 'AutoEncoder_model']:\n",
        "        model_file = os.path.join(models_path, f'{model_name}.pkl')\n",
        "        if os.path.exists(model_file):\n",
        "            try:\n",
        "                models[model_name] = joblib.load(model_file)\n",
        "                print(f\"Loaded {model_name} model.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {model_name} model: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"Model file {model_file} does not exist.\")\n",
        "    return models\n",
        "\n",
        "def process_file(file_path, contamination, models, dataset_type, value_type, value_column):\n",
        "    print(f\"Processing file: {file_path}\")\n",
        "    try:\n",
        "        # Read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], format='%d/%m/%Y %H:%M:%S')\n",
        "        df = df.sort_values('datetime')\n",
        "\n",
        "        print(f\"DataFrame shape: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns}\")\n",
        "\n",
        "        # Extract hour and day of week\n",
        "        df['hour'] = df['datetime'].dt.hour\n",
        "        df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "\n",
        "        # Calculate rolling statistics\n",
        "        window_size = 24 if dataset_type == 'helios' else 7\n",
        "        df['diff'] = df[value_column].diff()\n",
        "        df['rolling_mean'] = df[value_column].rolling(window=window_size).mean()\n",
        "        df['rolling_std'] = df[value_column].rolling(window=window_size).std()\n",
        "\n",
        "        # Calculate Z-scores\n",
        "        df['z_score'] = (df[value_column] - df['rolling_mean']) / df['rolling_std']\n",
        "\n",
        "        # Prepare features for anomaly detection\n",
        "        features = ['hour', 'day_of_week', 'rolling_mean', 'rolling_std']\n",
        "\n",
        "        # Handle NaN values\n",
        "        df[features] = df[features].fillna(df[features].mean())\n",
        "\n",
        "        X = df[features].values\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # Initialize results dictionary\n",
        "        results = {}\n",
        "\n",
        "        # Anomaly Detection Models\n",
        "        for model_name, model in models.items():\n",
        "            try:\n",
        "                outlier_scores = model.decision_function(X_scaled)\n",
        "                df[f'{model_name}_anomaly_score'] = outlier_scores\n",
        "                df[f'{model_name}_is_anomaly'] = model.predict(X_scaled)\n",
        "                z_score_threshold = 3 if dataset_type == 'helios' else 1\n",
        "                df[f'{model_name}_is_validated_anomaly'] = (df[f'{model_name}_is_anomaly'] == 1) & (abs(df['z_score']) > z_score_threshold)\n",
        "                results[model_name] = df[f'{model_name}_is_validated_anomaly'].sum()\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying {model_name} model: {str(e)}\")\n",
        "\n",
        "        # Create a new column for points that are anomalies according to all methods\n",
        "        df['all_methods_anomaly'] = df[[f'{model_name}_is_validated_anomaly' for model_name in models]].all(axis=1)\n",
        "        results['all_methods'] = df['all_methods_anomaly'].sum()\n",
        "\n",
        "        return df, results\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def plot_results(df, user_key, dataset_type, value_type, value_column, results):\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        x = df['datetime']\n",
        "        y = df[value_column]\n",
        "        title = f'Anomalies Detected by All Methods for {user_key}'\n",
        "\n",
        "        plt.plot(x, y, label='Consumption', alpha=0.5)\n",
        "\n",
        "        all_methods_anomalies = df[df['all_methods_anomaly']]\n",
        "        plt.scatter(all_methods_anomalies[x.name], all_methods_anomalies[y.name], label='Anomalies (All Methods)', color='red')\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.xlabel('DateTime')\n",
        "        plt.ylabel('Consumption')\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error plotting results: {str(e)}\")\n",
        "\n",
        "def main(folder_path, model_save_path, dataset_type, value_type, value_column, contamination=0.01):\n",
        "    print(f\"Starting main function with folder_path: {folder_path} and model_save_path: {model_save_path}\")\n",
        "\n",
        "    # Load pre-trained models\n",
        "    models = load_models(model_save_path)\n",
        "\n",
        "    # Check if models are loaded correctly\n",
        "    if not models:\n",
        "        print(\"No models loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        file_list = os.listdir(folder_path)\n",
        "        print(f\"Files in directory: {file_list}\")\n",
        "\n",
        "        for filename in file_list:\n",
        "            if filename.endswith('.csv'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "                df, results = process_file(file_path, contamination, models, dataset_type, value_type, value_column)\n",
        "\n",
        "                if df is not None and results is not None:\n",
        "                    user_key = df['user key'].iloc[0] if dataset_type == 'helios' else filename.split('.')[0]\n",
        "\n",
        "                    print(f\"Processing data for file: {filename}\")\n",
        "                    print(f\"Total data points: {len(df)}\")\n",
        "                    for model_name, count in results.items():\n",
        "                        print(f\"{model_name} validated anomalies: {count}\")\n",
        "\n",
        "                    # Plot the results\n",
        "                    plot_results(df, user_key, dataset_type, value_type, value_column, results)\n",
        "                else:\n",
        "                    print(f\"Skipping file {filename} due to processing error\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main function: {str(e)}\")\n",
        "\n",
        "# Get user input for dataset type and value type\n",
        "dataset_type = input(\"Enter dataset type (helios/queensland/datamill): \").lower()\n",
        "while dataset_type not in ['helios', 'queensland', 'datamill']:\n",
        "    dataset_type = input(\"Invalid input. Please enter 'helios', 'queensland', or 'datamill': \").lower()\n",
        "\n",
        "value_type = input(\"Enter data type (daily/total): \").lower()\n",
        "while value_type not in ['daily', 'total']:\n",
        "    value_type = input(\"Invalid input. Please enter 'daily' or 'total': \").lower()\n",
        "\n",
        "# Set the folder path, model save path, and value column based on the dataset type and value type\n",
        "if dataset_type == 'helios':\n",
        "    folder_path = './dataset/helios/user_helios_sorted'\n",
        "    model_save_path = f'./models/helios/{value_type}'\n",
        "    value_column = 'diff' if value_type == 'daily' else 'meter reading'\n",
        "elif dataset_type == 'queensland':\n",
        "    folder_path = f'./dataset/queensland/user_sorted_pulse{\"tot\" if value_type == \"total\" else \"\"}'\n",
        "    model_save_path = f'./models/queensland/{value_type}'\n",
        "    value_column = 'Pulse1' if value_type == 'daily' else 'Pulse1_Total'\n",
        "else:  # datamill\n",
        "    folder_path = './dataset/datamill/user_datamill_sorted'\n",
        "    model_save_path = f'./models/datamill/{value_type}'\n",
        "    value_column = 'DAILY_AVERAGE_CONSUMPTION' if value_type == 'daily' else 'GROSS_CONSUMPTION'\n",
        "\n",
        "print(f\"Selected dataset type: {dataset_type}\")\n",
        "print(f\"Selected value type: {value_type}\")\n",
        "print(f\"Value column: {value_column}\")\n",
        "print(f\"Folder path: {folder_path}\")\n",
        "print(f\"Model save path: {model_save_path}\")\n",
        "\n",
        "try:\n",
        "    main(folder_path, model_save_path, dataset_type, value_type, value_column, contamination=0.01)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "4LfBKgbIoFUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}